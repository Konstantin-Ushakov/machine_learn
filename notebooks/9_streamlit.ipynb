{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet, Ridge, SGDRegressor\n",
    "import pickle\n",
    "import streamlit as st\n",
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.activations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем предобработку данных.\n",
    "\n",
    "Разделим данные на обучение и тест, нормализуем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache\n",
    "def prepared_dataset(df, random_state=None):\n",
    "    X = df.drop('price', axis=1)\n",
    "    y = df.price.astype('int64')\n",
    "    numeric_cols = np.array(['symboling', 'normalized_losses', 'wheel_base', 'length', 'width',\n",
    "       'height', 'curb_weight', 'num_of_cylinders', 'engine_size', 'bore',\n",
    "       'stroke', 'compression_ratio', 'horsepower', 'peak_rpm', 'city_mpg',\n",
    "       'highway_mpg'])\n",
    "    X_numeric = X.loc[:, numeric_cols]\n",
    "    categorical_cols = list(set(X.columns.values.tolist()) - set(numeric_cols))\n",
    "    X_categorical = X.loc[:, categorical_cols]\n",
    "    for col in categorical_cols:\n",
    "        X_categorical[col] = X_categorical[col].astype('string')\n",
    "    # Разделим на обучение и тест\n",
    "    X_train, X_test, X_train_cat, X_test_cat, X_train_num, X_test_num, y_train, y_test = train_test_split(X, X_categorical, X_numeric, y, test_size=0.15, random_state=random_state)\n",
    "    # Номализуем значения\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_num)\n",
    "\n",
    "    X_train_num_sc = scaler.transform(X_train_num)\n",
    "    X_test_num_sc = scaler.transform(X_test_num)\n",
    "    # Соединяем значения воедино\n",
    "    X_train_transform = np.hstack((X_train_num_sc, X_train_cat))\n",
    "    X_test_transform = np.hstack((X_test_num_sc, X_test_cat))\n",
    "    return X_train_transform.astype('float32'), y_train.astype('float32'), X_test_transform.astype('float32'), y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(parameters={'input_shape':69, 'num_neurons':3, 'num_layers':1, 'activations':['relu'],\n",
    "                              'optimizer':'sgd', 'loss':'mse'}):\n",
    "    input_shape = parameters['input_shape']\n",
    "    num_neurons = parameters['num_neurons']\n",
    "    num_layers = parameters['num_layers']\n",
    "    activations = parameters['activations']\n",
    "    optimizer = parameters['optimizer']\n",
    "    loss = parameters['loss']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(num_neurons, activation=activations[i]))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(optimizer, loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейные алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявим доступные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model=\"\", parameters={}):\n",
    "    new_params = {}\n",
    "    if model == \"elastic_net\":\n",
    "        regressor = ElasticNet()\n",
    "    elif model == \"sgd_regressor\":\n",
    "        regressor = SGDRegressor()\n",
    "    elif model == \"ridge\":\n",
    "        regressor = Ridge()\n",
    "    elif model == 'neural_network':\n",
    "        return neural_network(parameters)\n",
    "    else:\n",
    "        regressor = ElasticNet()\n",
    "    # get all available parameters\n",
    "    available_params = set(regressor.get_params().keys()).intersection(set(parameters.keys()))\n",
    "    params = {a_p:parameters[a_p] for a_p in available_params}\n",
    "    regressor.set_params(**params)\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод <code>trainee_model</code> обучает модель с гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainee_model(X_train, y_train, model=\"\", parameters={'epochs':1}):\n",
    "    regressor = init_model(model, parameters)\n",
    "    if model == 'neural_network':\n",
    "        regressor.fit(X_train, y_train, epochs=parameters['epochs'])\n",
    "    else:\n",
    "        regressor.fit(X_train, y_train)\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод <code>process_prediction</code> выдает метрики качества для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    st.write('R^2: {}'.format(r2_score(y_test, y_pred)))\n",
    "    st.write('MSE: {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "    st.write('RMSE: {}'.format(mean_squared_error(y_test, y_pred, squared=False)))\n",
    "    st.write('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сериализация модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle [docs](https://docs.python.org/3/library/pickle.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сериализация в байтовое представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(model):\n",
    "    return pickle.dumps(model)\n",
    "\n",
    "def deserialize_model(bin_data):\n",
    "    return pickle.loads(bin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сериализации/десериализации в/из файла можно использовать следующие функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model_file(model, filename=\"ser.dat\"):\n",
    "    with open(filename, 'wb') as fin:\n",
    "        pickle.dump(model, fin)\n",
    "    return filename\n",
    "\n",
    "def deserialize_model_file(filename=\"ser.dat\"):\n",
    "    with open(filename, 'rb') as fout:\n",
    "        model = pickle.load(fout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [STREAMLIT статья](https://www.notion.so/a53e9e35dc4f482f889e2a3f516be9fd) [docs](https://docs.streamlit.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache\n",
    "def load_data(filename='../data/automobile_preprocessed.csv'):\n",
    "    df = pd.read_csv(filename, sep=';')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    page = st.sidebar.selectbox('Choose a page', ['Description', 'Regressor'])\n",
    "    \n",
    "    if page == 'Description':\n",
    "        st.header('Automobile dataset description')\n",
    "        description_task = '''## Task 3\n",
    "Подготовьте данные, уберите аномалии. Разбейте датасет, проведите обучение моделей: \n",
    "- Линейная регрессия;\n",
    "- Регрессия дерева решений;\n",
    "- LASSO;\n",
    "- Гребневая регрессия;\n",
    "- Elastic Net регрессия.\n",
    "\n",
    "Найдите реализации методов в sklearn, оставьте в нотбуке ссылки на документацию. Найдите наилучшие гиперпараметры. Оцените качество моделей: R2, Mean Square Error(MSE), Root Mean Square Error(RMSE),  mean absolute error (MAE). Свои действия снабжайте пояснениями.'''\n",
    "        st.write(description_task)\n",
    "        description = '''\n",
    "        ## Автомобили [Источник](https://archive.ics.uci.edu/ml/datasets/Automobile)\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "     Attribute:                Attribute Range:\n",
    "     ------------------        -----------------------------------------------\n",
    "  1. symboling:                -3, -2, -1, 0, 1, 2, 3.\n",
    "  2. normalized-losses:        continuous from 65 to 256.\n",
    "  3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
    "                               isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
    "                               mitsubishi, nissan, peugot, plymouth, porsche,\n",
    "                               renault, saab, subaru, toyota, volkswagen, volvo\n",
    "  4. fuel-type:                diesel, gas.\n",
    "  5. aspiration:               std, turbo.\n",
    "  6. num-of-doors:             four, two.\n",
    "  7. body-style:               hardtop, wagon, sedan, hatchback, convertible.\n",
    "  8. drive-wheels:             4wd, fwd, rwd.\n",
    "  9. engine-location:          front, rear.\n",
    " 10. wheel-base:               continuous from 86.6 120.9.\n",
    " 11. length:                   continuous from 141.1 to 208.1.\n",
    " 12. width:                    continuous from 60.3 to 72.3.\n",
    " 13. height:                   continuous from 47.8 to 59.8.\n",
    " 14. curb-weight:              continuous from 1488 to 4066.\n",
    " 15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
    " 16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n",
    " 17. engine-size:              continuous from 61 to 326.\n",
    " 18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
    " 19. bore:                     continuous from 2.54 to 3.94.\n",
    " 20. stroke:                   continuous from 2.07 to 4.17.\n",
    " 21. compression-ratio:        continuous from 7 to 23.\n",
    " 22. horsepower:               continuous from 48 to 288.\n",
    " 23. peak-rpm:                 continuous from 4150 to 6600.\n",
    " 24. city-mpg:                 continuous from 13 to 49.\n",
    " 25. highway-mpg:              continuous from 16 to 54.\n",
    " 26. price:                    continuous from 5118 to 45400.\n",
    "\n",
    "8. Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:   Number of instances missing a value:\n",
    "   2.             41\n",
    "   6.             2\n",
    "   19.            4\n",
    "   20.            4\n",
    "   22.            2\n",
    "   23.            2\n",
    "   26.            4\n",
    "\n",
    "### Source Information\n",
    "   -- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n",
    "   -- Date: 19 May 1987\n",
    "   -- Sources:\n",
    "     1) 1985 Model Import Car and Truck Specifications, 1985 Ward's\n",
    "        Automotive Yearbook.\n",
    "     2) Personal Auto Manuals, Insurance Services Office, 160 Water\n",
    "        Street, New York, NY 10038 \n",
    "     3) Insurance Collision Report, Insurance Institute for Highway\n",
    "        Safety, Watergate 600, Washington, DC 20037\n",
    "\n",
    "### Past Usage\n",
    "   -- Kibler,~D., Aha,~D.~W., \\& Albert,~M. (1989).  Instance-based prediction\n",
    "      of real-valued attributes.  {\\it Computational Intelligence}, {\\it 5},\n",
    "      51--57.\n",
    "\t -- Predicted price of car using all numeric and Boolean attributes\n",
    "\t -- Method: an instance-based learning (IBL) algorithm derived from a\n",
    "\t    localized k-nearest neighbor algorithm.  Compared with a\n",
    "\t    linear regression prediction...so all instances\n",
    "\t    with missing attribute values were discarded.  This resulted with\n",
    "\t    a training set of 159 instances, which was also used as a test\n",
    "\t    set (minus the actual instance during testing).\n",
    "\t -- Results: Percent Average Deviation Error of Prediction from Actual\n",
    "\t    -- 11.84% for the IBL algorithm\n",
    "\t    -- 14.12% for the resulting linear regression equation\n",
    "\n",
    "4. Relevant Information:\n",
    "   -- Description\n",
    "      This data set consists of three types of entities: (a) the\n",
    "      specification of an auto in terms of various characteristics, (b)\n",
    "      its assigned insurance risk rating, (c) its normalized losses in use\n",
    "      as compared to other cars.  The second rating corresponds to the\n",
    "      degree to which the auto is more risky than its price indicates.\n",
    "      Cars are initially assigned a risk factor symbol associated with its\n",
    "      price.   Then, if it is more risky (or less), this symbol is\n",
    "      adjusted by moving it up (or down) the scale.  Actuarians call this\n",
    "      process \"symboling\".  A value of +3 indicates that the auto is\n",
    "      risky, -3 that it is probably pretty safe.\n",
    "\n",
    "      The third factor is the relative average loss payment per insured\n",
    "      vehicle year.  This value is normalized for all autos within a\n",
    "      particular size classification (two-door small, station wagons,\n",
    "      sports/speciality, etc...), and represents the average loss per car\n",
    "      per year.\n",
    "\n",
    "   -- Note: Several of the attributes in the database could be used as a\n",
    "            \"class\" attribute.\n",
    "\n",
    "        '''\n",
    "        st.write(description)\n",
    "    else:\n",
    "        st.header('Automobile dataset')\n",
    "        df = load_data('../data/automobile_preprocessed.csv')\n",
    "        # Set model type\n",
    "        model_type = st.selectbox('Select model', ['elastic_net', 'ridge', 'sgd_regressor', 'neural_network'])\n",
    "        parameters = {}\n",
    "        \n",
    "        if model_type in ['elastic_net', 'ridge', 'sgd_regressor']:\n",
    "            alpha = st.slider('Select param alpha', min_value=0.0, max_value=3.0, value=1.0, step=0.1, key='alpha')\n",
    "            parameters['alpha'] = alpha\n",
    "            \n",
    "        if model_type == 'elastic_net':\n",
    "            fit_intercept = st.selectbox('Select fit intercept', [True, False], key='is_fit_intercept')\n",
    "            parameters['fit_intercept'] = fit_intercept\n",
    "            max_iter = st.slider('Select param max_iter', min_value=1, max_value=50, value=1, step=1, key='max_iter')\n",
    "            parameters['max_iter'] = max_iter\n",
    "            positive = st.selectbox('Select is all params are positive', [True, False], key='is_positive')\n",
    "            parameters['positive'] = positive\n",
    "        elif model_type == 'ridge':\n",
    "            normalize = st.selectbox('Select if normalize parameters', [True, False], key='is_normalize')\n",
    "            parameters['normalize'] = normalize\n",
    "        elif model_type == 'sgd_regressor':\n",
    "            penalty = st.selectbox('Select penalty', ['l2', 'l1', 'elasticnet'], key='penalty')\n",
    "            parameters['penalty'] = penalty\n",
    "            if penalty == 'elasticnet':\n",
    "                l1_ratio = st.slider('Select param l1_ratio', min_value=0.0, max_value=1.0, value=0.5, step=0.05, key='l1_ratio')\n",
    "                parameters['l1_ratio'] = l1_ratio\n",
    "        elif model_type == 'neural_network':\n",
    "            num_layers = st.slider('Select number of layers', min_value=1, max_value=3, step=1, key='num_layers')\n",
    "            parameters['num_layers'] = num_layers\n",
    "            activations = []\n",
    "            for i in range(num_layers):\n",
    "                activation = st.selectbox('Select activation for {} layer'.format(i + 1), \n",
    "                                          ['relu', 'sigmoid', 'softmax', 'softsign', 'tanh', 'selu', 'elu', 'exponential'], \n",
    "                                          key='activation_{}'.format(i))\n",
    "                activations.append(activation)\n",
    "            parameters['activations'] = activations\n",
    "            num_neurons = st.slider('Select number of neurons on each hidden layer', min_value=1, max_value=10, step=1, key='num_neurons')\n",
    "            parameters['num_neurons'] = num_neurons\n",
    "            optimizer = st.selectbox('Select optimizer function', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl'], key='optimizer')\n",
    "            parameters['optimizer'] = optimizer\n",
    "            loss = st.selectbox('Select loss function', ['mse', 'mae'], key='loss')\n",
    "            parameters['loss'] = loss\n",
    "            epochs = st.slider('Select number of epochs', min_value=1, max_value=1000, value=1, step=3, key='epochs')\n",
    "            parameters['epochs'] = epochs\n",
    "\n",
    "        X_train, y_train, X_test, y_test = prepared_dataset(df)\n",
    "        if model_type == 'neural_network':\n",
    "            input_shape = X_train.shape[1]\n",
    "            parameters['input_shape'] = input_shape\n",
    "        model = trainee_model(X_train, y_train, model=model_type, parameters=parameters)\n",
    "        if model_type != 'neural_network':\n",
    "            st.write(model)\n",
    "            # Сериализация модели\n",
    "            model_bytes = serialize_model(model)\n",
    "            # Десериализация модели\n",
    "            model_deser = deserialize_model(model_bytes)\n",
    "        process_prediction(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-19 16:50:57.695 WARNING root: \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run D:\\ProgramFiles\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
